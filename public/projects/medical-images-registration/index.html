<!doctype html><html lang=en dir=auto><head><script src="/JulienGenzling.github.io/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=JulienGenzling.github.io/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Medical images registration | JG</title>
<meta name=keywords content><meta name=description content="A key challenge in biomedical image analysis and particularly in brain image analysis is registration; human anatomical variability is the norm, necessitating the transformation of image data into standard coordinates. This standardization facilitates analysis and allows results to be generalized to a larger population.
Affine registration, which uses linear transformations, provides a robust initial alignment of overall brain structures. This is followed by diffeomorphic registration, which models smooth, non-linear deformations to capture intricate anatomical details."><meta name=author content="Julien Genzling"><link rel=canonical href=http://localhost:1313/JulienGenzling.github.io/projects/medical-images-registration/><link crossorigin=anonymous href=/JulienGenzling.github.io/assets/css/stylesheet.19834c6c40982c384456e404a11633a67d5dcaf6c3b20eb11d31d53e9a386681.css integrity="sha256-GYNMbECYLDhEVuQEoRYzpn1dyvbDsg6xHTHVPpo4ZoE=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/JulienGenzling.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/JulienGenzling.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/JulienGenzling.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/JulienGenzling.github.io/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/JulienGenzling.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/JulienGenzling.github.io/projects/medical-images-registration/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:title" content="Medical images registration"><meta property="og:description" content="A key challenge in biomedical image analysis and particularly in brain image analysis is registration; human anatomical variability is the norm, necessitating the transformation of image data into standard coordinates. This standardization facilitates analysis and allows results to be generalized to a larger population.
Affine registration, which uses linear transformations, provides a robust initial alignment of overall brain structures. This is followed by diffeomorphic registration, which models smooth, non-linear deformations to capture intricate anatomical details."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/JulienGenzling.github.io/projects/medical-images-registration/"><meta property="og:image" content="http://localhost:1313/JulienGenzling.github.io/projects/registration/cover.jpg"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-07-31T14:00:00+00:00"><meta property="article:modified_time" content="2024-07-31T14:00:00+00:00"><meta property="og:site_name" content="Julien Genzling"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/JulienGenzling.github.io/projects/registration/cover.jpg"><meta name=twitter:title content="Medical images registration"><meta name=twitter:description content="A key challenge in biomedical image analysis and particularly in brain image analysis is registration; human anatomical variability is the norm, necessitating the transformation of image data into standard coordinates. This standardization facilitates analysis and allows results to be generalized to a larger population.
Affine registration, which uses linear transformations, provides a robust initial alignment of overall brain structures. This is followed by diffeomorphic registration, which models smooth, non-linear deformations to capture intricate anatomical details."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/JulienGenzling.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Medical images registration","item":"http://localhost:1313/JulienGenzling.github.io/projects/medical-images-registration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Medical images registration","name":"Medical images registration","description":"A key challenge in biomedical image analysis and particularly in brain image analysis is registration; human anatomical variability is the norm, necessitating the transformation of image data into standard coordinates. This standardization facilitates analysis and allows results to be generalized to a larger population.\nAffine registration, which uses linear transformations, provides a robust initial alignment of overall brain structures. This is followed by diffeomorphic registration, which models smooth, non-linear deformations to capture intricate anatomical details.","keywords":[],"articleBody":"A key challenge in biomedical image analysis and particularly in brain image analysis is registration; human anatomical variability is the norm, necessitating the transformation of image data into standard coordinates. This standardization facilitates analysis and allows results to be generalized to a larger population.\nAffine registration, which uses linear transformations, provides a robust initial alignment of overall brain structures. This is followed by diffeomorphic registration, which models smooth, non-linear deformations to capture intricate anatomical details. Combining these methods ensures both global and local alignment, enhancing the correlation of structural and functional information and improving clinical and research outcomes.\nI’ll explain the theory behind affine and diffeomorphic registration, applied to 3D brain images registration, and their implementation in DIPY 0.\nAffine registration This part is based on the following paper : Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K., \u0026 Eubank, W. PET-CT image registration in the chest using free-form deformations. IEEE Transactions on Medical Imaging, 22(1), 120-8, 2003.\nIntroduction We assume that an image $I$ is described by a set of samples $I_i = I(\\mathbf{x}_i)$ where $\\mathbf{x}_i \\in \\Omega \\subseteq \\mathbb{R}^3$ defined on a cartesian grid with integer spacing.\nWe have a reference/fixed/static image $I_S$ and a moving image $I_M$. We want to register $I_M$ on $I_S$. We therefore need to find a function $\\mathbf{T}_{\\phi}$ describing the transformation from $\\Omega_M$ (codomain) to $\\Omega_S$ (domain), with $\\phi$ a set of transformation parameters to be determined. We can formulate this as an optimization problem. To align the reference image $I_S$ with the transformed image $\\mathbf{T}_{\\phi} \\circ I_M$, we seek the set of transformation parameters $\\phi$ that minimizes a discrepancy function $S$:\n$ \\phi^*= \\argmin\\limits_{\\phi} S(I_S, \\mathbf{T}_{\\phi} \\circ I_M) $\nIn the next sections we will answer to the following questions:\nWhat is $\\mathbf{T}_{\\phi}$ in the case of affine registration ? How is the registration implemented in practice (what is the discrepancy function $S$, what is the optimization scheme)? Affine registration parameters In 3D, an affine transformation can be represented by a $4 \\times 4$ matrix that contains 3 parameters for translations and 9 parameters for linear transformations (scaling, rotation, reflection, shear).\nThe following matrices constitute the basis affine transforms in 3D:\nTranslate: $\\begin{pmatrix} 1 \u0026 0 \u0026 0 \u0026 t_x\\\\ 0 \u0026 1 \u0026 0 \u0026 t_y\\\\ 0 \u0026 0 \u0026 1 \u0026 t_z\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\\\ \\end{pmatrix}$\nScale: $\\begin{pmatrix} s_x \u0026 0 \u0026 0 \u0026 0\\\\ 0 \u0026 s_y \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 s_z \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\\\ \\end{pmatrix}$\nShear: $\\begin{pmatrix} 1 \u0026 h_{xy} \u0026 h_{xz} \u0026 0\\\\ h_{yx} \u0026 1 \u0026 h_{yz} \u0026 0\\\\ h_{zx} \u0026 h_{zy} \u0026 1 \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\\\ \\end{pmatrix}$\n-Rotate along $x$ axis: $\\begin{pmatrix} 1 \u0026 0 \u0026 0 \u0026 0\\\\ 0 \u0026 \\cos(\\theta_x) \u0026 -\\sin(\\theta_x) \u0026 0\\\\ 0 \u0026 \\sin(\\theta_x) \u0026 \\cos(\\theta_x) \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\\\ \\end{pmatrix}$\n-Rotate along $y$ axis: $\\begin{pmatrix} \\cos(\\theta_y) \u0026 0 \u0026 \\sin(\\theta_y) \u0026 0\\\\ 0 \u0026 1 \u0026 0 \u0026 0\\\\ -\\sin(\\theta_y) \u0026 0 \u0026 \\cos(\\theta_y) \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\\\ \\end{pmatrix}$\n-Rotate along $z$ axis: $\\begin{pmatrix} \\cos(\\theta_z) \u0026 -\\sin(\\theta_z) \u0026 0 \u0026 0\\\\ \\sin(\\theta_z) \u0026 \\cos(\\theta_z) \u0026 0 \u0026 0\\\\ 0 \u0026 0 \u0026 1 \u0026 0\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\\\ \\end{pmatrix}$\nGiven a point $\\mathbf{x} = (x,y,z)^T$, its new position $\\mathbf{x’} = (x’,y’,z’)^T$ after an affine transformation will be:\n$ \\begin{pmatrix} x’\\\\ y’\\\\ z’\\\\ 1\\\\ \\end{pmatrix} = \\begin{pmatrix} a_0 \u0026 a_1 \u0026 a_2 \u0026 a_3\\\\ a_4 \u0026 a_5 \u0026 a_6 \u0026 a_7\\\\ a_8 \u0026 a_9 \u0026 a_{10} \u0026 a_{11}\\\\ 0 \u0026 0 \u0026 0 \u0026 1\\\\ \\end{pmatrix} \\begin{pmatrix} x’\\\\ y’\\\\ z’\\\\ 1\\\\ \\end{pmatrix} \\quad (1) $\nHere we have rephrased the basis affine transforms parameters in homogeneous coordinates, which allows us to combine the rotation/scale/sheer/translation in a single matrix.\nRegistration process and DIPY implementation The affine registration is implemented in DIPY in dipy/align/imaffine.py\nBrain MRI images format A common format for medical images, in our case, brain MRI images, is nifti. 3D images are stored in cartesian grids of shape $(q_x, q_y, q_z) \\in \\mathbb{N}^3$. An affine transform (matrix of shape $4 \\times 4$) is also stored. This matrix represents the voxel-to-world transformation. That is, if we apply $(1)$ to a voxel of the grid, the resulting coordinates will be the coordinates of this point but in the referential of the scanner ($x, y, z$ distance from the origin, which is located in the center of the brain, in mm) 1. We write $\\mathbf{A_S}$ for the domain voxel-to-world affine (static image) and $\\mathbf{A_M}$ for the codomain voxel-to-world affine (moving image).\nWhat does it mean to map a 3D image (=a grid/domain) on another 3D image ? The parameters of the transform $\\mathbf{T}_{\\phi}$ that we try to optimize come down to an affine matrix $\\mathbf{A}$ with $\\phi = (a_0, a_1, a_2, a_3, a_4, a_5, a_6, a_7, a_8, a_9, a_{10}, a_{11})$ the parameters of the affine transformation that we seek to optimize (see matrix in $(1)$).\nThe codomain $\\Omega_M$ (moving image) and the domain $\\Omega_S$ (static image)can have different shapes $(q_{M,x}, q_{M,y}, q_{M,z})$ and $(q_{S,x}, q_{S,y}, q_{S,z})$. If we register the moving image on the static image, the resulting moved image will have the shape of the static image. We therefore have a grid, the domain $\\Omega_S$, which we want to fill with values. For each voxel of the domain which we want to fill with a value, we fill it with the value that we retrieve in the codomain with the help of $\\mathbf{T}_{\\phi} = \\mathbf{A}$, $\\mathbf{A_S}$ and $\\mathbf{A_M}$.\nGiven $\\mathbf{x} \\in \\Omega_S = \\llbracket 0, q_{S,x} \\rrbracket \\times \\llbracket 0, q_{S,y} \\rrbracket \\times \\llbracket 0, q_{S,z} \\rrbracket$, we have:\n$\\underbrace{\\mathbf{T}_{\\phi}\\circ I_M}_{\\text{transformed image}} (\\mathbf{x}) = \\mathcal{L}(\\mathbf{A_M}^{-1}\\mathbf{A}\\mathbf{A_S}\\mathbf{x})$.\nThis means that in order to transform a moving image towards a static image, we first map each voxel $\\mathbf{x} = (i,j,k)$ of the static image to world coordinates $(x,y,z)$ by applying $\\mathbf{A_S}$. Then we apply the affine transform $\\mathbf{A}$ to $(x,y,z)$ obtaining $(x’, y’, z’)$ in moving image’s world coordinates. Finally, $(x’, y’, z’)$ is mapped to voxel coordinates $(i’, j’, k’)$ in the moving image by multiplying by the inverse of $\\mathbf{A_M}$.\n$\\mathcal{L}: \\mathbb{R}^3 \\longrightarrow \\mathbb{R}$ is a trilinear interpolation function 2. It linearly interpolates the values of the 8 voxels of $\\Omega_M$ that are the closest to $\\mathbf{A_M}^{-1}\\mathbf{A}\\mathbf{A_S}\\mathbf{x} \\in \\mathbb{R}^3$. This is useful when the codomain’s resolution is very small compared with the domain’s resolution ($q_M \\ll q_S$) as it allows to move the moving image in a smoother way into the domain.\nIn DIPY, this is done by the _apply_transform function in the AffineMap class.\nHow are the coefficients of $A$ computed ? The algorithmic core for computing and optimizing $S$ and finding the best matrix $A$ relies on Limited-memory BFGS 3, which is an optimization algorithm in the family of quasi-Newton methods.\nIn a classical Newton method, we would find $\\phi^*$ with the following iterative scheme :\n$ \\phi_{k+1} = \\phi_{k} - \\underbrace{[\\frac{\\partial^2S}{\\partial\\phi^2}]^{-1}}_{\\text{Hessian} \\in \\mathbb{R}^{12 \\times 12}} \\underbrace{\\frac{\\partial S}{\\partial\\phi}}_{\\text{Gradient} \\in \\mathbb{R}^{12}} = \\phi_{k} - [\\nabla^2S]^{-1}{\\nabla S} $\nHowever finding the inverse of the Hessian in high dimensions to compute the Newton direction can be an expensive operation. In quasi-Newton methods the Hessian matrix does not need to be computed. The Hessian is updated by analyzing successive gradient vectors instead.\nIn DIPY, this optimization is done in the optimize function of the ÀffineRegistration class.\nWhat is $S$ and how to compute $\\nabla S$ ? The image discrepancy measure used in DIPY is the negative mutual information. To understand mutual information, we need to define the joint histogram between $I_S$ and $\\mathbf{T}_{\\phi}\\circ I_M$. From now on, we consider note $I_T = \\mathbf{T}_{\\phi}\\circ I_M$ the transformed image. $I_S$ and $I_T$ are in the same space $\\Omega_S$ through the process that has been described before.\nAs implemented in DIPY, it is possible to use masks so that all voxels $\\mathbf{x}$ are not used to compute the mutual information between images. It is also possible to undersample (for instance only select 30% of the voxels). We therefore note $\\Omega$ the space of voxels that are used to actually compute the metric.\nLet $L_S$ and $L_T$ be specified numbers of uniformly sized bins along the respective dimensions of the joint histogram of the static and moving images. The joint histogram is a matrix $\\mathbf{H} \\in \\mathbb{N}^{L_S \\times L_T}$. The value $(a,b)$ in $\\mathbf{H}$ is equal to the number of voxels $\\mathbf{x} \\in \\Omega$ that have intensity $a$ in $I_S$ and intensity $b$ in $I_T$. By dividing by the total of number of pixels $|\\Omega|$, we normalize the histogram so that it behaves like a traditional probability distribution. The histogram bins are indexed by integer values $\\kappa$, $0 \\leq \\kappa \u003c L_S$ and $\\iota$, $0 \\leq \\iota \u003c L_T$.\nA Parzen window is used to generate continuous estimates of the underlying image distributions, thereby reducing the effects of quantization from interpolation and discretization from binning the data. It also makes the distributions differentiable which allows us to compute a gradient.\nLet $\\beta^{(3)}$ be a cubic spline Parzen window and $\\beta^{(0)}$ be a zero-order spline Parzen window (centered unit pulse). We will come bac on the calculation of the coefficients of the spline later. The smoothed joint histogram of $(I_S, I_T)$ is given by:\n$ p(\\iota, \\kappa | \\phi) = \\alpha \\displaystyle\\sum_{\\mathbf{x}\\in\\Omega}\\beta^{(0)}(\\kappa - \\frac{I_S(\\mathbf{x})-I_S^0}{\\Delta b_S}) \\times \\beta^{(3)}(\\iota - \\frac{I_T(\\mathbf{x})-I_T^0}{\\Delta b_T}) \\quad (2) $\nwhere $\\alpha$ is a normalization factor that ensures $\\sum p(\\iota, \\kappa) = 1$. Each contributing image value is normalized by the minimum intensity value, $I_S^0$ and $I_T^0$ and the intensity range of each bin, $\\Delta b_S$ and $\\Delta b_T$ to fit into the specified number of bins ($L_S$ or $L_T$) in the intensity distribution. The marginal smoothed histogram for the test image is computed from the joint histogram:\n$ p_T(\\iota | \\phi) = \\displaystyle\\sum_{\\kappa}p(\\iota, \\kappa | \\phi) $\nThe static image smoothed histogram is computed as:\n$ p_S(\\kappa) = \\alpha \\displaystyle\\sum_{\\mathbf{x}\\in \\Omega} \\beta^{(0)}(\\kappa - \\frac{I_S(\\mathbf{x})-I_S^0}{\\Delta b_S}) $\nThe negative of mutual information $S$ between the static image and the moved image is expressed as a function of the transformation parameters $\\phi$:\n$ S(I_S, \\mathbf{T}_{\\phi}\\circ I_M) = -\\displaystyle\\sum_{\\iota}\\displaystyle\\sum_{\\kappa}p(\\iota,\\kappa | \\phi)\\log \\frac{p(\\iota,\\kappa | \\phi)}{p_M(\\iota | \\phi)p_S(\\kappa)} $\nIt can be seen as the Kullback-Leibler divergence between $p$, the smoothed joint histogram, and $p_S \\otimes p_T$. It measures the cost for considering $I_T$ and $I_S$ as independant random variables, when in reality they are not.\nCalculation of the gradient of the cost function is necessary as seen earlier:\n$ \\nabla S = [\\frac{\\partial S}{\\partial a_0},…, \\frac{\\partial S}{\\partial a_{11}}]^T $\nA single component of the gradient requires differentiation of the joint distribution $(2)$. After applying the chain rule, the ith partial derivative of the joint distribution is given as:\n$ \\frac{\\partial p(\\iota, \\kappa)}{\\partial a_i} = \\frac{1}{\\Delta b_T |\\Omega|}\\displaystyle\\sum_{\\mathbf{x}\\in \\Omega}\\beta^{(0)}(\\kappa - \\frac{I_S(\\mathbf{x})-I_S^0}{\\Delta b_S}) \\times \\frac{\\partial \\beta^{(3)}(u)}{\\partial u}\\bigg|_{u=\\iota - \\frac{I_T(\\mathbf{x})-I_T^0}{\\Delta b_T}} \\times (\\frac{-\\partial I_M(\\mathbf{t})}{\\partial \\mathbf{t}} \\bigg|_{\\mathbf{t}=\\mathbf{T}_{\\phi}(\\mathbf{x})})^T \\frac{\\partial \\mathbf{T}_{\\phi}(\\mathbf{x})}{\\partial a_i} $\nIn DIPY, all the metric calculations of $S$ and $\\nabla S$ is done in the MutualInformationMetric class. In our case, the transform argument is the 3D affine transformation.\nB-Spline coefficients calculation As explained in the introduction, $I$ is described by a set of samples $I_i = I(\\mathbf{x}_i)$ where $\\mathbf{x}_i \\in \\Omega$.\nThe calculation of $I(\\mathbf{x})$ at points not on the grid requires an interpolation method based on the samples $I_i$ and their locations $\\mathbf{x}_i$. In DIPY, an interpolation scheme taht represents the underlying continuous image by a B-Spline basis is used. The expansion of coefficients $c_i$ of the basis are computed from the image samples $I_i$ through an efficient recursive filtering algorithm [4]. Values of $I(\\mathbf{x})$ that do not lie on the lattice can be interpolated:\n$ I(\\mathbf{x}) = \\displaystyle\\sum_{i}c_i \\beta^{(3)}(\\mathbf{x}-\\mathbf{x_i}) $\nwhere $\\mathbf{x} = (x,y,z)^T$ is any real-valued voxel location in the volume, $\\mathbf{x_i} = (x_i,y_i,z_i)^T$ is the coordinate vector of a lattice point, and $\\beta^{(3)}(\\mathbf{x})= \\beta^{(3)}(x)\\beta^{(3)}(y)\\beta^{(3)}(z)$ is a separable convolution kernel. The argument of the spline window is the sampled cubic B-Spline:\n$ \\beta^{(3)}(x) = \\begin{cases} \\frac{1}{6}(4-6x^2+3|x|^3) \u0026 \\text{if } 0 \\leq |x| \u003c 1, \\\\ \\frac{1}{6}(2-|x|)^3 \u0026 \\text{if } 1 \\leq |x| \u003c 2, \\\\ 0 \u0026 \\text{if } 2 \\leq |x|. \\end{cases} $\nThe gradient of the interpolated image at any location can therefore be calculated :\n$ \\frac{\\partial I(\\mathbf{x})}{\\partial x} = \\displaystyle\\sum_{i}c_i(\\frac{d\\beta^{(3)}(u)}{du}\\bigg|_{u=x-x_i}\\beta^{(3)}(y-y_i)\\beta^{(3)}(z-z_i)) $\nwith similar formulas for $\\frac{\\partial I}{\\partial y}$ and $\\frac{\\partial I}{\\partial z}$. The cubic spline window $\\beta^{(3)}$ can be differentiated explicitely and after simplification redices to the difference of two shifted second-order splines:\n$ \\frac{d\\beta^{(3)}(u)}{du} = \\beta^{(2)}(u+\\frac{1}{2}) - \\beta^{(2)}(u-\\frac{1}{2}) $\nMultiresolution framework In order to avoid local minima and to decrease computatio time, a hierarchical multiresolution optimization scheme is used. This means that $\\mathbf{A}$ is initially calculated for our images with a downsized resolution, then as the resolution is increased, fine misalignments are recovered. The low resolution images are smoothed with a gaussian kernel.\nIn DIPY, this corresponds to the level_iters, sigmas and factors argument that are used to initialize the ÀffineRegistration class. The level_iters argument is the number of iterations at each scale of the scale space (by default, a 3-level scale space with iterations sequence equal to $[10000, 1000, 100]$ is used). The sigmas argument is a custom smoothing parameter to build the scale space (standard deviation of the smoothing gaussian kernels, defaults to $[3,1,0]$). The factors argument defines the resolution of the scale spaces at each level($\\Omega$), it defaults to $[4,2,1]$.\nImage affine registration result example You can see on this overlay that after affine registration, both brains are aligned but we can still see some spots where they need to be morphed in a non-linear way. That’s what is explained in the next part.\nDiffeomorphic map registration A diffeomorphism is an invertible and differentiable function whose inverse is also differentiable. The warp registration is implemented in DIPY in dipy/align/imwarp.py\nThe diffeomorphism $\\Psi$ is implemented by means of a deformation field $\\psi$ that assigns to each point $\\mathbf{x}$ a displacement vector $\\psi(\\mathbf{x})$ such that $\\Psi(\\mathbf{x}) = \\mathbf{x} + \\psi(\\mathbf{x})$. In DIPY, the diffeomorphic map (implemented in the DiffeomorphicMap class) includes a pre-alignment matrix $\\mathbf{P}$ so that $\\Psi(\\mathbf{x}) = \\mathbf{Px} + \\psi(\\mathbf{Px})$. This matrix is the affine matrix that was calculated in the last section.\nThe static and moving images are projected into a discretized deformation field (the shape of the displacement field is the disp_shape argument in the DiffeomorphicMap class, which usually is the shape of the domain).\nThe following diagram provides an overview of the registration process (taken from 5).\nHere the objective is to calculate the function $f$ on the diagram.\nSymmetric Diffeomorphic Registration The greedy algorithm for Symmetric Diffeomorphic Registration (“Greedy SyN”) 6 finds a diffeomorphism mapping back and forth between two given images by looking for two diffeomorphisms mapping the given images to an “intermediate” shape and then composing the intermediate mappings to find the diffeomorphism between the two original images.\nBoth displacement fields (forward and backward) have the same discretization (same grid shape and grid-to-space transform), which means that the deformation fields actually define endomorphisms. The domain of these endomorphisms (equal to their codomain) is called “reference domain”, and similarly, their discretization grid is called “reference grid”. For convenience, the reference discretization (grid shape and grid-tospace transform) is arbitrarily chosen to be the same as the static image. As a consequence, the prealigning matrix corresponding to the static-to-reference diffeomorphism ($\\Psi_1$ in the above figure) is the identity (only the moving image is pre-aligned to the reference)\nThe following is an overview of the Greedy SyN algorithm:\nHere is a subsample of the forward warp field (the grid is the displacement field grid).\nAnd the backward warp field:\nImage affine + warp registration result example You can see on this overlay that after affine and warp registration, both brains are very well aligned.\nThe result usually is even better is we get rid of the skull (skullstripping step) before doing the registration:\nRSL registration code We implemented a complete registration code that handles all of these steps (skullstripping, affine registration, warp registration) in the following github repository : https://github.com/rauschecker-sugrue-labs/rsl-register.\nYou can register two images by launching the following command :\npython register.py fixed_image_path moving_image_path output_directory --already_skullstripped --registration_type --overwrite --log_path --verbosity --nthreads --nickname --affine_transform_path --warp_transform_path --plot References [4]: M. Unser, A. Aldroubi, and M. Eden, “Fast B-spline transforms for continuous image representation and interpolation,” IEEE Trans. Pattern Anal. Machine Intell., vol. 13, pp. 277–285, Mar. 1991.\n","wordCount":"2733","inLanguage":"en","image":"http://localhost:1313/JulienGenzling.github.io/projects/registration/cover.jpg","datePublished":"2024-07-31T14:00:00Z","dateModified":"2024-07-31T14:00:00Z","author":{"@type":"Person","name":"Julien Genzling"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/JulienGenzling.github.io/projects/medical-images-registration/"},"publisher":{"@type":"Organization","name":"JG","logo":{"@type":"ImageObject","url":"http://localhost:1313/JulienGenzling.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/JulienGenzling.github.io/ accesskey=h title="Julien Genzling (Alt + H)"><img src=http://localhost:1313/JulienGenzling.github.io/taupek.jpeg alt aria-label=logo height=30>Julien Genzling</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/JulienGenzling.github.io/paper-reviews/ title="Paper Reviews"><span>Paper Reviews</span></a></li><li><a href=http://localhost:1313/JulienGenzling.github.io/gallery/ title="Art gallery"><span>Art gallery</span></a></li><li><a href=http://localhost:1313/JulienGenzling.github.io/projects/ title=Projects><span>Projects</span></a></li></ul></nav></header><style>.nav{display:flex;justify-content:space-between;align-items:center;max-width:1200px;margin:0 auto;padding:0 20px}.logo{display:flex;align-items:center}.logo-img{width:24px;height:24px;margin-right:5px}.logo-img{width:24px;height:24px;border-radius:15%;margin-right:5px;transition:transform 1s cubic-bezier(.25,1,.5,1)}.logo-img:hover{animation:spin 1s cubic-bezier(.25,1,.5,1)forwards}@keyframes spin{0%{transform:rotate(0)}80%{transform:rotate(355)}100%{transform:rotate(360deg)}}.logo-switches{margin-left:20px}</style><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/JulienGenzling.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/JulienGenzling.github.io/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">Medical images registration</h1><div class=post-meta><span title='2024-07-31 14:00:00 +0000 UTC'>July 31, 2024</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Julien Genzling</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/JulienGenzling.github.io/projects/registration/cover.jpg alt="source : https://viso.ai/computer-vision/image-registration/"></figure><div class=post-content><p>A key challenge in biomedical image analysis and particularly in brain image analysis is <strong>registration</strong>; human anatomical variability is the norm, necessitating the transformation of image data into standard coordinates. This standardization facilitates analysis and allows results to be generalized to a larger population.</p><p><strong>Affine registration</strong>, which uses linear transformations, provides a robust initial alignment of overall brain structures. This is followed by <strong>diffeomorphic registration</strong>, which models smooth, non-linear deformations to capture intricate anatomical details. Combining these methods ensures both global and local alignment, enhancing the correlation of structural and functional information and improving clinical and research outcomes.</p><p>I&rsquo;ll explain the theory behind affine and diffeomorphic registration, applied to 3D brain images registration, and their implementation in <code>DIPY</code> <a href=https://dipy.org/>0</a>.</p><h2 id=affine-registration>Affine registration<a hidden class=anchor aria-hidden=true href=#affine-registration>#</a></h2><p>This part is based on the following paper : <em>Mattes, D., Haynor, D. R., Vesselle, H., Lewellen, T. K., & Eubank</em>, W. PET-CT image registration in the chest using free-form deformations. IEEE Transactions on Medical Imaging, 22(1), 120-8, 2003.</p><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>We assume that an image $I$ is described by a set of samples $I_i = I(\mathbf{x}_i)$ where $\mathbf{x}_i \in \Omega \subseteq \mathbb{R}^3$ defined on a cartesian grid with integer spacing.</p><p>We have a reference/fixed/static image $I_S$ and a moving image $I_M$. We want to register $I_M$ on $I_S$. We therefore need to find a function $\mathbf{T}_{\phi}$ describing the transformation from $\Omega_M$ (codomain) to $\Omega_S$ (domain), with $\phi$ a set of transformation parameters to be determined. We can formulate this as an optimization problem. To align the reference image $I_S$ with the transformed image $\mathbf{T}_{\phi} \circ I_M$, we seek the set of transformation parameters $\phi$ that minimizes a discrepancy function $S$:</p><p>$
\phi^*= \argmin\limits_{\phi} S(I_S, \mathbf{T}_{\phi} \circ I_M)
$</p><p>In the next sections we will answer to the following questions:</p><ul><li>What is $\mathbf{T}_{\phi}$ in the case of affine registration ?</li><li>How is the registration implemented in practice (what is the discrepancy function $S$, what is the optimization scheme)?</li></ul><h3 id=affine-registration-parameters>Affine registration parameters<a hidden class=anchor aria-hidden=true href=#affine-registration-parameters>#</a></h3><p>In 3D, an affine transformation can be represented by a $4 \times 4$ matrix that contains 3 parameters for translations and 9 parameters for linear transformations (scaling, rotation, reflection, shear).</p><p>The following matrices constitute the basis affine transforms in 3D:</p><ul><li><p>Translate: $\begin{pmatrix}
1 & 0 & 0 & t_x\\
0 & 1 & 0 & t_y\\
0 & 0 & 1 & t_z\\
0 & 0 & 0 & 1\\
\end{pmatrix}$</p></li><li><p>Scale: $\begin{pmatrix}
s_x & 0 & 0 & 0\\
0 & s_y & 0 & 0\\
0 & 0 & s_z & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}$</p></li><li><p>Shear: $\begin{pmatrix}
1 & h_{xy} & h_{xz} & 0\\
h_{yx} & 1 & h_{yz} & 0\\
h_{zx} & h_{zy} & 1 & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}$</p></li></ul><p>-Rotate along $x$ axis: $\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & \cos(\theta_x) & -\sin(\theta_x) & 0\\
0 & \sin(\theta_x) & \cos(\theta_x) & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}$</p><p>-Rotate along $y$ axis: $\begin{pmatrix}
\cos(\theta_y) & 0 & \sin(\theta_y) & 0\\
0 & 1 & 0 & 0\\
-\sin(\theta_y) & 0 & \cos(\theta_y) & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}$</p><p>-Rotate along $z$ axis: $\begin{pmatrix}
\cos(\theta_z) & -\sin(\theta_z) & 0 & 0\\
\sin(\theta_z) & \cos(\theta_z) & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1\\
\end{pmatrix}$</p><p>Given a point $\mathbf{x} = (x,y,z)^T$, its new position $\mathbf{x&rsquo;} = (x&rsquo;,y&rsquo;,z&rsquo;)^T$ after an affine transformation will be:</p><p>$
\begin{pmatrix}
x&rsquo;\\
y&rsquo;\\
z&rsquo;\\
1\\
\end{pmatrix} = \begin{pmatrix}
a_0 & a_1 & a_2 & a_3\\
a_4 & a_5 & a_6 & a_7\\
a_8 & a_9 & a_{10} & a_{11}\\
0 & 0 & 0 & 1\\
\end{pmatrix} \begin{pmatrix}
x&rsquo;\\
y&rsquo;\\
z&rsquo;\\
1\\
\end{pmatrix}
\quad (1)
$</p><p>Here we have rephrased the basis affine transforms parameters in homogeneous coordinates, which allows us to combine the rotation/scale/sheer/translation in a single matrix.</p><h3 id=registration-process-and-dipy-implementation>Registration process and DIPY implementation<a hidden class=anchor aria-hidden=true href=#registration-process-and-dipy-implementation>#</a></h3><p>The affine registration is implemented in <code>DIPY</code> in <code>dipy/align/imaffine.py</code></p><h4 id=brain-mri-images-format>Brain MRI images format<a hidden class=anchor aria-hidden=true href=#brain-mri-images-format>#</a></h4><p>A common format for medical images, in our case, brain MRI images, is <code>nifti</code>. 3D images are stored in cartesian grids of shape $(q_x, q_y, q_z) \in \mathbb{N}^3$. An affine transform (matrix of shape $4 \times 4$) is also stored. This matrix represents the voxel-to-world transformation. That is, if we apply $(1)$ to a voxel of the grid, the resulting coordinates will be the coordinates of this point but in the referential of the scanner ($x, y, z$ distance from the origin, which is located in the center of the brain, in mm) <a href=https://nipy.org/nibabel/coordinate_systems.html>1</a>. We write $\mathbf{A_S}$ for the domain voxel-to-world affine (static image) and $\mathbf{A_M}$ for the codomain voxel-to-world affine (moving image).</p><h4 id=what-does-it-mean-to-map-a-3d-image-a-griddomain-on-another-3d-image->What does it mean to map a 3D image (=a grid/domain) on another 3D image ?<a hidden class=anchor aria-hidden=true href=#what-does-it-mean-to-map-a-3d-image-a-griddomain-on-another-3d-image->#</a></h4><p>The parameters of the transform $\mathbf{T}_{\phi}$ that we try to optimize come down to an affine matrix $\mathbf{A}$ with $\phi = (a_0, a_1, a_2, a_3, a_4, a_5, a_6, a_7, a_8, a_9, a_{10}, a_{11})$ the parameters of the affine transformation that we seek to optimize (see matrix in $(1)$).</p><p>The codomain $\Omega_M$ (moving image) and the domain $\Omega_S$ (static image)can have different shapes $(q_{M,x}, q_{M,y}, q_{M,z})$ and $(q_{S,x}, q_{S,y}, q_{S,z})$. If we register the moving image on the static image, the resulting moved image will have the shape of the static image. We therefore have a grid, the domain $\Omega_S$, which we want to fill with values. For each voxel of the domain which we want to fill with a value, we fill it with the value that we retrieve in the codomain with the help of $\mathbf{T}_{\phi} = \mathbf{A}$, $\mathbf{A_S}$ and $\mathbf{A_M}$.</p><p>Given $\mathbf{x} \in \Omega_S = \llbracket 0, q_{S,x} \rrbracket \times \llbracket 0, q_{S,y} \rrbracket \times \llbracket 0, q_{S,z} \rrbracket$, we have:</p><p>$\underbrace{\mathbf{T}_{\phi}\circ I_M}_{\text{transformed image}} (\mathbf{x}) = \mathcal{L}(\mathbf{A_M}^{-1}\mathbf{A}\mathbf{A_S}\mathbf{x})$.</p><p>This means that in order to transform a moving image towards a static image, we first map each voxel $\mathbf{x} = (i,j,k)$ of the static image to world coordinates $(x,y,z)$ by applying $\mathbf{A_S}$. Then we apply the affine transform $\mathbf{A}$ to $(x,y,z)$ obtaining $(x&rsquo;, y&rsquo;, z&rsquo;)$ in moving image&rsquo;s world coordinates. Finally, $(x&rsquo;, y&rsquo;, z&rsquo;)$ is mapped to voxel coordinates $(i&rsquo;, j&rsquo;, k&rsquo;)$ in the moving image by multiplying by the inverse of $\mathbf{A_M}$.</p><p>$\mathcal{L}: \mathbb{R}^3 \longrightarrow \mathbb{R}$ is a trilinear interpolation function <a href=https://en.wikipedia.org/wiki/Trilinear_interpolation>2</a>. It linearly interpolates the values of the 8 voxels of $\Omega_M$ that are the closest to $\mathbf{A_M}^{-1}\mathbf{A}\mathbf{A_S}\mathbf{x} \in \mathbb{R}^3$. This is useful when the codomain&rsquo;s resolution is very small compared with the domain&rsquo;s resolution ($q_M \ll q_S$) as it allows to move the moving image in a smoother way into the domain.</p><p>In <code>DIPY</code>, this is done by the <code>_apply_transform</code> function in the <code>AffineMap</code> class.</p><h4 id=how-are-the-coefficients-of-a-computed->How are the coefficients of $A$ computed ?<a hidden class=anchor aria-hidden=true href=#how-are-the-coefficients-of-a-computed->#</a></h4><p>The algorithmic core for computing and optimizing $S$ and finding the best matrix $A$ relies on <strong>Limited-memory BFGS</strong> <a href=https://en.wikipedia.org/wiki/Limited-memory_BFGS>3</a>, which is an optimization algorithm in the family of quasi-Newton methods.</p><p>In a classical Newton method, we would find $\phi^*$ with the following iterative scheme :</p><p>$
\phi_{k+1} = \phi_{k} - \underbrace{[\frac{\partial^2S}{\partial\phi^2}]^{-1}}_{\text{Hessian} \in \mathbb{R}^{12 \times 12}} \underbrace{\frac{\partial S}{\partial\phi}}_{\text{Gradient} \in \mathbb{R}^{12}} = \phi_{k} - [\nabla^2S]^{-1}{\nabla S}
$</p><p>However finding the inverse of the Hessian in high dimensions to compute the Newton direction can be an expensive operation. In quasi-Newton methods the Hessian matrix does not need to be computed. The Hessian is updated by analyzing successive gradient vectors instead.</p><p>In <code>DIPY</code>, this optimization is done in the <code>optimize</code> function of the <code>ÀffineRegistration</code> class.</p><h4 id=what-is-s-and-how-to-compute-nabla-s->What is $S$ and how to compute $\nabla S$ ?<a hidden class=anchor aria-hidden=true href=#what-is-s-and-how-to-compute-nabla-s->#</a></h4><p>The image discrepancy measure used in <code>DIPY</code> is the <strong>negative mutual information</strong>.
To understand mutual information, we need to define the joint histogram between $I_S$ and $\mathbf{T}_{\phi}\circ I_M$.
From now on, we consider note $I_T = \mathbf{T}_{\phi}\circ I_M$ the transformed image. $I_S$ and $I_T$ are in the same space $\Omega_S$ through the process that has been described before.</p><p>As implemented in <code>DIPY</code>, it is possible to use masks so that all voxels $\mathbf{x}$ are not used to compute the mutual information between images. It is also possible to undersample (for instance only select 30% of the voxels). We therefore note $\Omega$ the space of voxels that are used to actually compute the metric.</p><p>Let $L_S$ and $L_T$ be specified numbers of uniformly sized bins
along the respective dimensions of the joint histogram of the
static and moving images. The joint histogram is a matrix $\mathbf{H} \in \mathbb{N}^{L_S \times L_T}$. The value $(a,b)$ in $\mathbf{H}$ is equal to the number of voxels $\mathbf{x} \in \Omega$ that have intensity $a$ in $I_S$ and intensity $b$ in $I_T$. By dividing by the total of number of pixels $|\Omega|$, we normalize the histogram so that it behaves like a traditional probability distribution. The histogram bins are indexed by integer values $\kappa$, $0 \leq \kappa &lt; L_S$ and $\iota$, $0 \leq \iota &lt; L_T$.</p><p>A Parzen window is used to generate continuous estimates of the underlying image distributions, thereby reducing the effects of quantization from interpolation and discretization from binning the data. It also makes the distributions differentiable which allows us to compute a gradient.</p><p>Let $\beta^{(3)}$ be a cubic spline Parzen window and $\beta^{(0)}$ be a zero-order spline Parzen window (centered unit pulse). We will come bac on the calculation of the coefficients of the spline later. The smoothed joint histogram of $(I_S, I_T)$ is given by:</p><p>$
p(\iota, \kappa | \phi) = \alpha \displaystyle\sum_{\mathbf{x}\in\Omega}\beta^{(0)}(\kappa - \frac{I_S(\mathbf{x})-I_S^0}{\Delta b_S}) \times \beta^{(3)}(\iota - \frac{I_T(\mathbf{x})-I_T^0}{\Delta b_T}) \quad (2)
$</p><p>where $\alpha$ is a normalization factor that ensures $\sum p(\iota, \kappa) = 1$. Each contributing image value is normalized by the minimum intensity value, $I_S^0$ and $I_T^0$ and the intensity range of each bin, $\Delta b_S$ and $\Delta b_T$ to fit into the specified number of bins ($L_S$ or $L_T$) in the intensity distribution.
The marginal smoothed histogram for the test image is computed from the joint histogram:</p><p>$
p_T(\iota | \phi) = \displaystyle\sum_{\kappa}p(\iota, \kappa | \phi)
$</p><p>The static image smoothed histogram is computed as:</p><p>$
p_S(\kappa) = \alpha \displaystyle\sum_{\mathbf{x}\in \Omega} \beta^{(0)}(\kappa - \frac{I_S(\mathbf{x})-I_S^0}{\Delta b_S})
$</p><p>The <strong>negative of mutual information $S$</strong> between the static image and the moved image is expressed as a function of the transformation parameters $\phi$:</p><p>$
S(I_S, \mathbf{T}_{\phi}\circ I_M) = -\displaystyle\sum_{\iota}\displaystyle\sum_{\kappa}p(\iota,\kappa | \phi)\log \frac{p(\iota,\kappa | \phi)}{p_M(\iota | \phi)p_S(\kappa)}
$</p><p>It can be seen as the Kullback-Leibler divergence between $p$, the smoothed joint histogram, and $p_S \otimes p_T$. It measures the cost for considering $I_T$ and $I_S$ as independant random variables, when in reality they are not.</p><p>Calculation of the gradient of the cost function is necessary as seen earlier:</p><p>$
\nabla S = [\frac{\partial S}{\partial a_0},&mldr;, \frac{\partial S}{\partial a_{11}}]^T
$</p><p>A single component of the gradient requires differentiation of the joint distribution $(2)$. After applying the chain rule, the ith partial derivative of the joint distribution is given as:</p><p>$
\frac{\partial p(\iota, \kappa)}{\partial a_i} = \frac{1}{\Delta b_T |\Omega|}\displaystyle\sum_{\mathbf{x}\in \Omega}\beta^{(0)}(\kappa - \frac{I_S(\mathbf{x})-I_S^0}{\Delta b_S}) \times \frac{\partial \beta^{(3)}(u)}{\partial u}\bigg|_{u=\iota - \frac{I_T(\mathbf{x})-I_T^0}{\Delta b_T}} \times (\frac{-\partial I_M(\mathbf{t})}{\partial \mathbf{t}} \bigg|_{\mathbf{t}=\mathbf{T}_{\phi}(\mathbf{x})})^T \frac{\partial \mathbf{T}_{\phi}(\mathbf{x})}{\partial a_i}
$</p><p>In <code>DIPY</code>, all the metric calculations of $S$ and $\nabla S$ is done in the <code>MutualInformationMetric</code> class. In our case, the <code>transform</code> argument is the 3D affine transformation.</p><h4 id=b-spline-coefficients-calculation>B-Spline coefficients calculation<a hidden class=anchor aria-hidden=true href=#b-spline-coefficients-calculation>#</a></h4><p>As explained in the introduction, $I$ is described by a set of samples $I_i = I(\mathbf{x}_i)$ where $\mathbf{x}_i \in \Omega$.</p><p>The calculation of $I(\mathbf{x})$ at points not on the grid requires an interpolation method based on the samples $I_i$ and their locations $\mathbf{x}_i$. In <code>DIPY</code>, an interpolation scheme taht represents the underlying continuous image by a <strong>B-Spline basis</strong> is used. The expansion of coefficients $c_i$ of the basis are computed from the image samples $I_i$ through an efficient recursive filtering algorithm [4]. Values of $I(\mathbf{x})$ that do not lie on the lattice can be interpolated:</p><p>$
I(\mathbf{x}) = \displaystyle\sum_{i}c_i \beta^{(3)}(\mathbf{x}-\mathbf{x_i})
$</p><p>where $\mathbf{x} = (x,y,z)^T$ is any real-valued voxel location in the volume, $\mathbf{x_i} = (x_i,y_i,z_i)^T$ is the coordinate vector of a lattice point, and $\beta^{(3)}(\mathbf{x})= \beta^{(3)}(x)\beta^{(3)}(y)\beta^{(3)}(z)$ is a separable convolution kernel. The argument of the spline window is the sampled cubic B-Spline:</p><p>$
\beta^{(3)}(x) = \begin{cases}
\frac{1}{6}(4-6x^2+3|x|^3) & \text{if } 0 \leq |x| &lt; 1, \\
\frac{1}{6}(2-|x|)^3 & \text{if } 1 \leq |x| &lt; 2, \\
0 & \text{if } 2 \leq |x|.
\end{cases}
$</p><p>The gradient of the interpolated image at any location can therefore be calculated :</p><p>$
\frac{\partial I(\mathbf{x})}{\partial x} = \displaystyle\sum_{i}c_i(\frac{d\beta^{(3)}(u)}{du}\bigg|_{u=x-x_i}\beta^{(3)}(y-y_i)\beta^{(3)}(z-z_i))
$</p><p>with similar formulas for $\frac{\partial I}{\partial y}$ and $\frac{\partial I}{\partial z}$. The cubic spline window $\beta^{(3)}$ can be differentiated explicitely and after simplification redices to the difference of two shifted second-order splines:</p><p>$
\frac{d\beta^{(3)}(u)}{du} = \beta^{(2)}(u+\frac{1}{2}) - \beta^{(2)}(u-\frac{1}{2})
$</p><h4 id=multiresolution-framework>Multiresolution framework<a hidden class=anchor aria-hidden=true href=#multiresolution-framework>#</a></h4><p>In order to avoid local minima and to decrease computatio time, a hierarchical multiresolution optimization scheme is used. This means that $\mathbf{A}$ is initially calculated for our images with a downsized resolution, then as the resolution is increased, fine misalignments are recovered. The low resolution images are smoothed with a gaussian kernel.</p><p>In <code>DIPY</code>, this corresponds to the <code>level_iters</code>, <code>sigmas</code> and <code>factors</code> argument that are used to initialize the <code>ÀffineRegistration</code> class. The <code>level_iters</code> argument is the number of iterations at each scale of the scale space (by default, a 3-level scale space with iterations sequence equal to $[10000, 1000, 100]$ is used). The <code>sigmas</code> argument is a custom smoothing parameter to build the scale space (standard deviation of the smoothing gaussian kernels, defaults to $[3,1,0]$). The <code>factors</code> argument defines the resolution of the scale spaces at each level($\Omega$), it defaults to $[4,2,1]$.</p><h4 id=image-affine-registration-result-example>Image affine registration result example<a hidden class=anchor aria-hidden=true href=#image-affine-registration-result-example>#</a></h4><p><img loading=lazy src=../../projects/registration/affine_moving.png alt=affine_reg></p><p>You can see on this overlay that after affine registration, both brains are aligned but we can still see some spots where they need to be morphed in a <strong>non-linear</strong> way. That&rsquo;s what is explained in the next part.</p><h2 id=diffeomorphic-map-registration>Diffeomorphic map registration<a hidden class=anchor aria-hidden=true href=#diffeomorphic-map-registration>#</a></h2><p>A diffeomorphism is an invertible and differentiable function whose inverse is also differentiable. The warp registration is implemented in <code>DIPY</code> in <code>dipy/align/imwarp.py</code></p><p>The diffeomorphism $\Psi$ is implemented by means of a deformation field $\psi$ that assigns to each point $\mathbf{x}$ a displacement vector $\psi(\mathbf{x})$ such that $\Psi(\mathbf{x}) = \mathbf{x} + \psi(\mathbf{x})$. In <code>DIPY</code>, the diffeomorphic map (implemented in the <code>DiffeomorphicMap</code> class) includes a pre-alignment matrix $\mathbf{P}$ so that $\Psi(\mathbf{x}) = \mathbf{Px} + \psi(\mathbf{Px})$. This matrix is the affine matrix that was calculated in the last section.</p><p>The static and moving images are projected into a discretized deformation field (the shape of the displacement field is the <code>disp_shape</code> argument in the <code>DiffeomorphicMap</code> class, which usually is the shape of the domain).</p><p>The following diagram provides an overview of the registration process (taken from <a href=https://www.cimat.mx/~omar/syn/tutorial.pdf>5</a>).</p><p><img loading=lazy src=../../projects/registration/warping.png alt=warping></p><p>Here the objective is to calculate the function $f$ on the diagram.</p><h3 id=symmetric-diffeomorphic-registration>Symmetric Diffeomorphic Registration<a hidden class=anchor aria-hidden=true href=#symmetric-diffeomorphic-registration>#</a></h3><p>The greedy algorithm for Symmetric Diffeomorphic Registration (“<strong>Greedy SyN</strong>”) <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2276735/>6</a> finds a diffeomorphism mapping back and forth between two given images by looking for two diffeomorphisms mapping the given images to an “intermediate” shape and then composing the intermediate mappings to find the diffeomorphism between the two original images.</p><p>Both displacement fields (forward and backward) have the same discretization (same grid shape and grid-to-space transform), which means that the deformation fields actually define endomorphisms. The domain of these endomorphisms (equal to their codomain) is called “reference domain”, and similarly, their discretization grid is called “reference grid”.
For convenience, the reference discretization (grid shape and grid-tospace transform) is arbitrarily chosen to be the same as the static image.
As a consequence, the prealigning matrix corresponding to the static-to-reference diffeomorphism ($\Psi_1$ in the above figure) is the identity (only the moving image is pre-aligned to the reference)</p><p><img loading=lazy src=../../projects/registration/syn.png alt=warping></p><p>The following is an overview of the Greedy SyN algorithm:</p><p><img loading=lazy src=../../projects/registration/syn-algo.png alt=warping></p><p>Here is a subsample of the forward warp field (the grid is the displacement field grid).</p><p><img loading=lazy src=../../projects/registration/forward_warp.png alt=warping></p><p>And the backward warp field:</p><p><img loading=lazy src=../../projects/registration/backward_warp.png alt=warping></p><h4 id=image-affine--warp-registration-result-example>Image affine + warp registration result example<a hidden class=anchor aria-hidden=true href=#image-affine--warp-registration-result-example>#</a></h4><p><img loading=lazy src=../../projects/registration/warped_moving.png alt=warp_reg></p><p>You can see on this overlay that after affine and warp registration, both brains are very well aligned.</p><p>The result usually is even better is we get rid of the skull (skullstripping step) before doing the registration:</p><p><img loading=lazy src=../../projects/registration/affine_moving_skullstripped.png alt=affine_reg_skull></p><p><img loading=lazy src=../../projects/registration/warped_moving_skullstripped.png alt=warp_reg_skull></p><h2 id=rsl-registration-code>RSL registration code<a hidden class=anchor aria-hidden=true href=#rsl-registration-code>#</a></h2><p>We implemented a complete registration code that handles all of these steps (skullstripping, affine registration, warp registration) in the following github repository : <a href=https://github.com/rauschecker-sugrue-labs/rsl-register>https://github.com/rauschecker-sugrue-labs/rsl-register</a>.</p><p>You can register two images by launching the following command :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python register.py fixed_image_path
</span></span><span style=display:flex><span>moving_image_path 
</span></span><span style=display:flex><span>output_directory 
</span></span><span style=display:flex><span>--already_skullstripped 
</span></span><span style=display:flex><span>--registration_type 
</span></span><span style=display:flex><span>--overwrite 
</span></span><span style=display:flex><span>--log_path 
</span></span><span style=display:flex><span>--verbosity 
</span></span><span style=display:flex><span>--nthreads 
</span></span><span style=display:flex><span>--nickname 
</span></span><span style=display:flex><span>--affine_transform_path
</span></span><span style=display:flex><span>--warp_transform_path 
</span></span><span style=display:flex><span>--plot
</span></span></code></pre></div><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[4]: M. Unser, A. Aldroubi, and M. Eden, <em>“Fast B-spline transforms for continuous image representation and interpolation,”</em> IEEE Trans. Pattern
Anal. Machine Intell., vol. 13, pp. 277–285, Mar. 1991.</p></div><footer class=post-footer><ul class=post-tags></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Medical images registration on x" href="https://x.com/intent/tweet/?text=Medical%20images%20registration&amp;url=http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Medical images registration on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f&amp;title=Medical%20images%20registration&amp;summary=Medical%20images%20registration&amp;source=http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Medical images registration on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f&title=Medical%20images%20registration"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Medical images registration on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Medical images registration on whatsapp" href="https://api.whatsapp.com/send?text=Medical%20images%20registration%20-%20http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Medical images registration on telegram" href="https://telegram.me/share/url?text=Medical%20images%20registration&amp;url=http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Medical images registration on ycombinator" href="https://news.ycombinator.com/submitlink?t=Medical%20images%20registration&u=http%3a%2f%2flocalhost%3a1313%2fJulienGenzling.github.io%2fprojects%2fmedical-images-registration%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/JulienGenzling.github.io/>JG</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>